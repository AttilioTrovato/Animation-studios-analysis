{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "ignore = ['ti','510','phi__','eh','lo','tis','ho','917','72nd','120pluspound','300plus','bbecause','ohohoho','coo','woohoowooooooo','wooooo','shalalalalala','woohoohoohoooo','woohoooo','woohoowoohoooo','60s','208','mmkay','ok__','pta','umm','ja4ll','2nd','lve','uq','8th','240','ri','433','pah','ohho','oww','cos','td','illumination','r1734','z7829','dvr','1415','99','hmph','hee','bev','rds','mmmmhmm','22nd','lu','rev','uhh','unh','mmhmm','whoo','300','uhhuh','hhhow','de','30','ohh','aaaaaahhhhh','aaaaaaahhhhh ','aa','heh','ugh','300 ','quite','mmm','sort','actually','cause','gonna','lad','shh','ha','uh','ah','ahhh','ooh','ya', 'so', 'got', 'that', 'this', '000', 'em', 'huh', 'aye', 'dum', 'la', 'ssh', 'okay', 'ok', 'gotta', 'hmm', 'aw', 'ow', 'also', 'yes', 'ah', 'said', 'well', 'would', 'yeah', 'two', 'shut','till', 'shall','john', 'ya', 'gotta', 'bit', 'hi', 'outta', 'bye', 'ii', 'aah','um', 'whoa', 'wanna','wow','uh']\n",
    "for x in ignore:\n",
    "    STOPWORDS.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1940-disney = 0\n",
    "# 1950-disney = 1\n",
    "# 1960-disney = 2\n",
    "# 1970-disney = 3\n",
    "# 1980-disney = 4\n",
    "# 1990-disney = 5\n",
    "# 2000-disney = 6\n",
    "# 2010-disney = 7\n",
    "\n",
    "# 1980-ghibli = 8\n",
    "# 1990-ghibli = 9\n",
    "# 2000-ghibli = 10\n",
    "# 2010-ghibli = 11\n",
    "\n",
    "# 2000-aardman = 12\n",
    "# 2010-aardman = 13\n",
    "\n",
    "# 2000-blue = 14\n",
    "# 2010-blue = 15\n",
    "\n",
    "# 2010-illumination = 16\n",
    "\n",
    "# 1990-dreamworks = 17\n",
    "# 2000-dreamworks = 18\n",
    "# 2010-dreamworks = 19\n",
    "\n",
    "# 2010-paramount = 20\n",
    "\n",
    "# 2000-sony = 21\n",
    "# 2010-sony = 22\n",
    "\n",
    "# 1990-animation = 23\n",
    "# 2000-animation = 24\n",
    "# 2010-animation = 25\n",
    "\n",
    "# 1950-disney-liveaction = 26\n",
    "# 1960-disney-liveaction = 27\n",
    "# 1970-disney-liveaction = 28\n",
    "# 1980-disney-liveaction = 29\n",
    "# 1990-disney-liveaction = 30\n",
    "# 2000-disney-liveaction = 31\n",
    "# 2010-disney-liveaction = 32\n",
    "\n",
    "# 1940-disney-liveaction+animation = 34\n",
    "# 1960-disney-liveaction+animation = 35\n",
    "# 1970-disney-liveaction+animation = 36\n",
    "# 1980-disney-liveaction+animation = 37\n",
    "# 1990-disney-liveaction+animation = 38\n",
    "# 2000-disney-liveaction+animation = 39\n",
    "# 2010-disney-liveaction+animation = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "t = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432\n",
      "432\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\Attilio\\Desktop\\progettone\\word cloud subtitles\\clean_subtitles(CORRETTI)\\Disney\\Animation + Live Action\\2010'\n",
    "\n",
    "duemila_disney_file = []\n",
    "for x, y, z in os.walk(path):\n",
    "    for a in z:\n",
    "      #  if \".srt\" in a:\n",
    "        duemila_disney_file.append(x+'\\\\'+a)\n",
    "\n",
    "\n",
    "for srt_file in duemila_disney_file:\n",
    "    with open(srt_file,encoding=\"utf-8\", errors = 'ignore') as f:\n",
    "        srt_file_as_string = f.read()\n",
    "    data.append(srt_file_as_string)\n",
    "    t.append(40)\n",
    "print(len(data))\n",
    "print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data\n",
    "t1 = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tutti_i_film_1.txt', 'w',encoding=\"utf-8\", errors = 'ignore' ) as f:\n",
    "    for item in data1:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        f.write(\"##############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('target.txt', 'w',encoding=\"utf-8\", errors = 'ignore' ) as g:\n",
    "    for item_1 in t1:\n",
    "        g.write(\"%s\\n\" % item_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duemila_disney_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codice alejandro correlazione\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from tsr_functions import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_words(X, y, nwords, featnames, tsr_function=information_gain_positive):\n",
    "    nD,nF = X.shape\n",
    "    positives = y.sum()\n",
    "    negatives = nD - positives\n",
    "\n",
    "    # computes the 4-cell contingency tables for each feature\n",
    "    TP = np.asarray((X[y==1]>0).sum(axis=0)).flatten()\n",
    "    FN = positives - TP\n",
    "    FP = np.asarray((X[y==0]>0).sum(axis=0)).flatten()\n",
    "    TN = negatives - FP\n",
    "    _4cell = [ContTable(tp=TP[i], tn=TN[i], fp=FP[i], fn=FN[i]) for i in range(nF)]\n",
    "\n",
    "    # applies the tsr_function to the 4-cell counters\n",
    "    feat_informativeness = np.array(list(map(tsr_function, _4cell)))\n",
    "\n",
    "    top_relevant_terms = np.argsort(-feat_informativeness)[:nwords]\n",
    "    feat_names = np.asarray(featnames)[top_relevant_terms]\n",
    "    feat_informativeness = feat_informativeness[top_relevant_terms]\n",
    "\n",
    "    return list(zip(feat_names, feat_informativeness))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data\n",
    "target = np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Attilio\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aaaaaaahhhhh'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, stop_words = STOPWORDS)\n",
    "X = tfidf.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(432, 41)\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(target.reshape(-1, 1))\n",
    "nC = Y.shape[1]\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 done\n",
      "class 1 done\n",
      "class 2 done\n",
      "class 3 done\n",
      "class 4 done\n",
      "class 5 done\n",
      "class 6 done\n",
      "class 7 done\n",
      "class 8 done\n",
      "class 9 done\n",
      "class 10 done\n",
      "class 11 done\n",
      "class 12 done\n",
      "class 13 done\n",
      "class 14 done\n",
      "class 15 done\n",
      "class 16 done\n",
      "class 17 done\n",
      "class 18 done\n",
      "class 19 done\n",
      "class 20 done\n",
      "class 21 done\n",
      "class 22 done\n",
      "class 23 done\n",
      "class 24 done\n",
      "class 25 done\n",
      "class 26 done\n",
      "class 27 done\n",
      "class 28 done\n",
      "class 29 done\n",
      "class 30 done\n",
      "class 31 done\n",
      "class 32 done\n",
      "class 33 done\n",
      "class 34 done\n",
      "class 35 done\n",
      "class 36 done\n",
      "class 37 done\n",
      "class 38 done\n",
      "class 39 done\n",
      "class 40 done\n"
     ]
    }
   ],
   "source": [
    "nwords = 100\n",
    "for i in range(nC):\n",
    "    # getting the 10 most relevant terms for each class according to the (positive only) correlation to the\n",
    "    # group label, as quantified by information gain\n",
    "    class_i_relevant = get_relevant_words(X, Y[:,i], nwords, tfidf.get_feature_names())\n",
    "    one_doc_as_df = pd.DataFrame.from_records(class_i_relevant)\n",
    "    one_doc_as_df.to_csv(f'class {i}.txt', header = None, index = False)\n",
    "    #print(f'class {i}: ' + ', '.join([f'{word} ({rel:.3f})' for word, rel in class_i_relevant]))\n",
    "    #print('#############################################')\n",
    "    print(f'class {i} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_doc_as_df.to_csv('prova.txt', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(class_i_relevant,  columns=['term', 'score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')  + ['shh', 'hoo', 'boo', 'uhoh', 'aah', 'heh', 'huh', 'ooh', 'yo', 'uh', 'um', 'aaah']\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "from collections import Counter\n",
    "import spacy.cli\n",
    "spacy.cli.download('en')\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/angelo/Desktop/master_offline/progettone_github/Animation-studios-analysis/sottotitoli/clean_subtitles/Disney/Animation/\"\n",
    "\n",
    "all_text = []\n",
    "for r, d, files in os.walk(path):\n",
    "    for f in files:\n",
    "        filename = r+'/'+f\n",
    "        with open(filename, 'r', encoding ='utf-8', errors='ignore') as fh:\n",
    "            text = fh.read()\n",
    "            all_text.append(text)\n",
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_document = ' '.join(all_text).lower()\n",
    "one_document = one_document.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy sentences splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sents(text):\n",
    "    text_split = []\n",
    "    tmp_text = text\n",
    "    while len(tmp_text) > 1000000:\n",
    "        split_pos = tmp_text.find('.', 900000)\n",
    "        text_split.append(tmp_text[:split_pos+1])\n",
    "        tmp_text = tmp_text[split_pos+1:]\n",
    "\n",
    "    text_split.append(tmp_text)\n",
    "    \n",
    "    sentences = []\n",
    "    for t in text_split:\n",
    "        print(len(t))\n",
    "        doc = nlp(t) \n",
    "        sentences = sentences + [s.text for s in doc.sents]\n",
    "        \n",
    "    return sentences  \n",
    "\n",
    "sentences = tokenize_sents(one_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter = Counter([len(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126021"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh, the world will sing of an english king a thousand years from now and not because he passed some laws or had that lofty brow while bonny good king richard leads the great crusade he's on we'll all have to slave away for that goodfornothin'john incredible as he is inept whenever the history books are kept they'll call him the phoney king of england a pox on the phoney king of england he sits alone on a giant throne pretendin'he's the king a little tyke who's rather like a puppet on a string\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    if len(s) == 497:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK sentences splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = sent_tokenize(one_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418222"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'long ago, in the faraway land of ancient greece, there was a golden age of powerful gods and extraordinary heroes.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_sents = [[word for word in word_tokenize(sentence) if (word.isalpha())] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long',\n",
       " 'ago',\n",
       " 'in',\n",
       " 'the',\n",
       " 'faraway',\n",
       " 'land',\n",
       " 'of',\n",
       " 'ancient',\n",
       " 'greece',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'golden',\n",
       " 'age',\n",
       " 'of',\n",
       " 'powerful',\n",
       " 'gods',\n",
       " 'and',\n",
       " 'extraordinary',\n",
       " 'heroes']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(tok_sents, size=100, window=10, min_count=15, sg=0, iter=20, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2724"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learned vocabulary\n",
    "words = list(w2v_model.wv.vocab)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ew', 0.5347276329994202),\n",
       " ('written', 0.5124107003211975),\n",
       " ('yay', 0.512235164642334),\n",
       " ('secure', 0.5073331594467163),\n",
       " ('guilty', 0.5066348314285278),\n",
       " ('shining', 0.5009575486183167),\n",
       " ('harmony', 0.4992484152317047),\n",
       " ('sticks', 0.48156318068504333),\n",
       " ('lily', 0.48082229495048523),\n",
       " ('darkness', 0.47160327434539795)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(['cinderella'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6090882420539856),\n",
       " ('aurora', 0.5444124937057495),\n",
       " ('gods', 0.5437379479408264),\n",
       " ('gift', 0.5090900659561157),\n",
       " ('hail', 0.5060056447982788),\n",
       " ('england', 0.49288177490234375),\n",
       " ('kingdom', 0.4925558269023895),\n",
       " ('warriors', 0.491148442029953),\n",
       " ('arendelle', 0.47572800517082214),\n",
       " ('evil', 0.46845555305480957)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(['king', 'princess'], ['prince'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43243396"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('king', 'majesty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2724"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = set(w2v_model.wv.vocab)\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2543"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = voc.difference(en_stopwords)\n",
    "voc = set([w for w in voc if len(w) > 2])\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "indici = dict()\n",
    "for index, key in enumerate(voc):\n",
    "    if key not in indici:\n",
    "        indici[key] = index\n",
    "        \n",
    "reverse_indici = dict()\n",
    "for term, termID in indici.items():\n",
    "    reverse_indici[termID] = term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2543"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#costruzione matrice\n",
    "graph_matrix = np.zeros(shape=(len(voc), len(voc)), dtype=np.float32)\n",
    "\n",
    "for index1, key1 in enumerate(voc):\n",
    "    for index2, key2 in enumerate(voc):\n",
    "        graph_matrix[index1 ][index2] = w2v_model.wv.similarity(key1, key2)\n",
    "        \n",
    "len(graph_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes: 309\n",
      "links: 426\n"
     ]
    }
   ],
   "source": [
    "#constructing the list of tuples\n",
    "graph_list = []\n",
    "nodes = set() # <-- NUOVO\n",
    "for x in range(len(graph_matrix)):\n",
    "    for y in range(x+1, len(graph_matrix)):\n",
    "        if graph_matrix[x][y] >= 0.7 and x != y:\n",
    "            nodes |= set([reverse_indici[x], reverse_indici[y]])\n",
    "            graph_list.append((reverse_indici[x],reverse_indici[y],graph_matrix[x][y]))      \n",
    "print('nodes:', len(nodes))\n",
    "print('links:', len(graph_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUOVO\n",
    "graph = dict()\n",
    "graph[\"nodes\"] = [{\"id\": t} for t in nodes]\n",
    "graph[\"links\"] = [{\"source\":t[0], \"target\":t[1], \"value\":float(t[2])} for t in graph_list]\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(graph, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fast_model = FastText(tok_sents,size=100, window=10, min_count=5, sg=1, iter=20, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.8033797740936279),\n",
       " ('appleby', 0.761911153793335),\n",
       " ('applesauce', 0.7409085631370544),\n",
       " ('appleseed', 0.700799822807312),\n",
       " ('dumplings', 0.6254189014434814),\n",
       " ('dumpling', 0.591278076171875),\n",
       " ('applejack', 0.5866483449935913),\n",
       " ('pickles', 0.5391175150871277),\n",
       " ('pies', 0.5119739770889282),\n",
       " ('pickle', 0.5112180709838867)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_model.wv.most_similar(['apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rolvaag', 0.6693047881126404),\n",
       " ('queen', 0.6678218841552734),\n",
       " ('norway', 0.5872023105621338),\n",
       " ('maldonia', 0.5784778594970703),\n",
       " ('france', 0.5775133371353149),\n",
       " ('ulstead', 0.5696249008178711),\n",
       " ('shanyu', 0.5687069892883301),\n",
       " ('tudor', 0.5649985671043396),\n",
       " ('renaldi', 0.564649224281311),\n",
       " ('narnia', 0.5620883107185364)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_model.wv.most_similar(['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mirrors', 0.8397473096847534),\n",
       " ('horror', 0.5931379199028015),\n",
       " ('error', 0.541779637336731),\n",
       " ('maggot', 0.5126214623451233),\n",
       " ('drawer', 0.509195864200592),\n",
       " ('carpet', 0.5068209171295166),\n",
       " ('gazelle', 0.5040353536605835),\n",
       " ('closet', 0.5015039443969727),\n",
       " ('window', 0.49620378017425537),\n",
       " ('spider', 0.49421101808547974)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_model.wv.most_similar(['mirror'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fa89631733d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfasttext_wAp_analogy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_word_analogies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy4unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_file' is not defined"
     ]
    }
   ],
   "source": [
    "fasttext_wAp_analogy = fast_model.wv.evaluate_word_analogies(test_file, dummy4unknown=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# google precomputed word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "google_w2v_file = 'GoogleNews-vectors-negative300.bin'\n",
    "google_model = gensim.models.KeyedVectors.load_word2vec_format(google_w2v_file,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model.most_similar(['king','female'],['male'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_large_analogy = google_model.wv.accuracy(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

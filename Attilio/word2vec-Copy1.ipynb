{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/angelo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')  + ['shh', 'hoo', 'boo', 'uhoh', 'aah', 'heh', 'huh', 'ooh', 'yo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/angelo/Desktop/master_offline/progettone_github/Animation-studios-analysis/sottotitoli/clean_subtitles/Disney/Animation/\"\n",
    "\n",
    "all_text = []\n",
    "for r, d, files in os.walk(path):\n",
    "    for f in files:\n",
    "        filename = r+'/'+f\n",
    "        with open(filename, 'r', encoding ='utf-8', errors='ignore') as fh:\n",
    "            text = fh.read()\n",
    "            all_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_document = ' '.join(all_text).lower()\n",
    "one_document = one_document.replace('\\n', ' ')\n",
    "sentences = sent_tokenize(one_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111766"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'long ago, in the faraway land of ancient greece, there was a golden age of powerful gods and extraordinary heroes.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_sents = [[word for word in word_tokenize(sentence) if (word.isalpha() and word not in en_stopwords)] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long',\n",
       " 'ago',\n",
       " 'faraway',\n",
       " 'land',\n",
       " 'ancient',\n",
       " 'greece',\n",
       " 'golden',\n",
       " 'age',\n",
       " 'powerful',\n",
       " 'gods',\n",
       " 'extraordinary',\n",
       " 'heroes']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['long',\n",
       "  'ago',\n",
       "  'faraway',\n",
       "  'land',\n",
       "  'ancient',\n",
       "  'greece',\n",
       "  'golden',\n",
       "  'age',\n",
       "  'powerful',\n",
       "  'gods',\n",
       "  'extraordinary',\n",
       "  'heroes'],\n",
       " ['greatest', 'strongest', 'heroes', 'mighty', 'hercules'],\n",
       " ['measure', 'true', 'hero']]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sents[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter([len(s) for s in sentences])\n",
    "sorted(counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"you're jokin'  you're jokin'  i can't believe  my eyes  you're jokin' me  you gotta be  this can't be  the right guy  he's ancient  he's ugly  i don't know  which is worse  i might just split a seam now  if i don't die laughin' first  mr. oogie boogie says  there's trouble close at hand  you better  pay attention now  'cause i'm  the boogie man  and if you  aren't shakin'  there's somethin'  very wrong  'cause this may be the last time  you hear the boogie song  whoaoh  wahah  whoaho  ow  whoaoh  wow  i'm the oogie boogie man  release me now or you must  face the dire consequences  the children are expecting me  so please, come to your senses  ah, you're jokin'  you're jokin'  i can't believe  my ears  would someone  shut this fella up  i'm drownin'  in my tears  he's funny  i'm laughin'  you really are  too much  and now  with your permission  i'm goin' to do my stuff  what are you going to do?\"]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in sentences if len(s) == 908]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-13 01:05:01,135 : INFO : collecting all words and their counts\n",
      "2020-06-13 01:05:01,137 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-06-13 01:05:01,154 : INFO : PROGRESS: at sentence #10000, processed 54385 words, keeping 5208 word types\n",
      "2020-06-13 01:05:01,171 : INFO : PROGRESS: at sentence #20000, processed 110999 words, keeping 8498 word types\n",
      "2020-06-13 01:05:01,180 : INFO : PROGRESS: at sentence #30000, processed 164371 words, keeping 10730 word types\n",
      "2020-06-13 01:05:01,189 : INFO : PROGRESS: at sentence #40000, processed 218007 words, keeping 12266 word types\n",
      "2020-06-13 01:05:01,201 : INFO : PROGRESS: at sentence #50000, processed 273017 words, keeping 14271 word types\n",
      "2020-06-13 01:05:01,211 : INFO : PROGRESS: at sentence #60000, processed 325122 words, keeping 15843 word types\n",
      "2020-06-13 01:05:01,221 : INFO : PROGRESS: at sentence #70000, processed 375894 words, keeping 17106 word types\n",
      "2020-06-13 01:05:01,232 : INFO : PROGRESS: at sentence #80000, processed 427353 words, keeping 18644 word types\n",
      "2020-06-13 01:05:01,241 : INFO : PROGRESS: at sentence #90000, processed 475753 words, keeping 19554 word types\n",
      "2020-06-13 01:05:01,251 : INFO : PROGRESS: at sentence #100000, processed 531218 words, keeping 20458 word types\n",
      "2020-06-13 01:05:01,262 : INFO : PROGRESS: at sentence #110000, processed 583780 words, keeping 21681 word types\n",
      "2020-06-13 01:05:01,264 : INFO : collected 21806 word types from a corpus of 592830 raw words and 111766 sentences\n",
      "2020-06-13 01:05:01,265 : INFO : Loading a fresh vocabulary\n",
      "2020-06-13 01:05:01,276 : INFO : effective_min_count=15 retains 2725 unique words (12% of original 21806, drops 19081)\n",
      "2020-06-13 01:05:01,277 : INFO : effective_min_count=15 leaves 540045 word corpus (91% of original 592830, drops 52785)\n",
      "2020-06-13 01:05:01,283 : INFO : deleting the raw counts dictionary of 21806 items\n",
      "2020-06-13 01:05:01,284 : INFO : sample=0.001 downsamples 69 most-common words\n",
      "2020-06-13 01:05:01,284 : INFO : downsampling leaves estimated 371420 word corpus (68.8% of prior 540045)\n",
      "2020-06-13 01:05:01,289 : INFO : estimated required memory for 2725 words and 200 dimensions: 5722500 bytes\n",
      "2020-06-13 01:05:01,290 : INFO : resetting layer weights\n",
      "2020-06-13 01:05:01,633 : INFO : training model with 3 workers on 2725 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=10 window=10\n",
      "2020-06-13 01:05:01,991 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:01,992 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:01,996 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:01,996 : INFO : EPOCH - 1 : training on 592830 raw words (371579 effective words) took 0.4s, 1041623 effective words/s\n",
      "2020-06-13 01:05:02,398 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:02,401 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:02,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:02,404 : INFO : EPOCH - 2 : training on 592830 raw words (371566 effective words) took 0.4s, 929747 effective words/s\n",
      "2020-06-13 01:05:02,852 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:02,855 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:02,861 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:02,862 : INFO : EPOCH - 3 : training on 592830 raw words (371437 effective words) took 0.4s, 827563 effective words/s\n",
      "2020-06-13 01:05:03,318 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:03,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:03,324 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:03,325 : INFO : EPOCH - 4 : training on 592830 raw words (371872 effective words) took 0.5s, 817695 effective words/s\n",
      "2020-06-13 01:05:03,730 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:03,731 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:03,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:03,735 : INFO : EPOCH - 5 : training on 592830 raw words (371640 effective words) took 0.4s, 922246 effective words/s\n",
      "2020-06-13 01:05:04,151 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:04,152 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:04,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:04,157 : INFO : EPOCH - 6 : training on 592830 raw words (371455 effective words) took 0.4s, 895397 effective words/s\n",
      "2020-06-13 01:05:04,578 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:04,580 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:04,586 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:04,586 : INFO : EPOCH - 7 : training on 592830 raw words (371351 effective words) took 0.4s, 877559 effective words/s\n",
      "2020-06-13 01:05:04,961 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:04,966 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:04,969 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:04,969 : INFO : EPOCH - 8 : training on 592830 raw words (371530 effective words) took 0.4s, 986539 effective words/s\n",
      "2020-06-13 01:05:05,354 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:05,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:05,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:05,358 : INFO : EPOCH - 9 : training on 592830 raw words (371571 effective words) took 0.4s, 971717 effective words/s\n",
      "2020-06-13 01:05:05,812 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:05,814 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:05,820 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:05,820 : INFO : EPOCH - 10 : training on 592830 raw words (371968 effective words) took 0.5s, 816337 effective words/s\n",
      "2020-06-13 01:05:06,247 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:06,250 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:06,261 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:06,262 : INFO : EPOCH - 11 : training on 592830 raw words (371434 effective words) took 0.4s, 856344 effective words/s\n",
      "2020-06-13 01:05:06,745 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:06,751 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:06,755 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:06,755 : INFO : EPOCH - 12 : training on 592830 raw words (371421 effective words) took 0.5s, 785133 effective words/s\n",
      "2020-06-13 01:05:07,188 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:07,189 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:07,193 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:07,193 : INFO : EPOCH - 13 : training on 592830 raw words (371591 effective words) took 0.4s, 862030 effective words/s\n",
      "2020-06-13 01:05:07,599 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:07,600 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:07,602 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:07,603 : INFO : EPOCH - 14 : training on 592830 raw words (371503 effective words) took 0.4s, 922820 effective words/s\n",
      "2020-06-13 01:05:08,013 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:08,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:08,027 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:08,028 : INFO : EPOCH - 15 : training on 592830 raw words (371339 effective words) took 0.4s, 888773 effective words/s\n",
      "2020-06-13 01:05:08,455 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:08,458 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:08,459 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:08,459 : INFO : EPOCH - 16 : training on 592830 raw words (371000 effective words) took 0.4s, 883333 effective words/s\n",
      "2020-06-13 01:05:08,889 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:08,890 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:08,895 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:08,896 : INFO : EPOCH - 17 : training on 592830 raw words (371113 effective words) took 0.4s, 867443 effective words/s\n",
      "2020-06-13 01:05:09,323 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:09,324 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:09,330 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:09,330 : INFO : EPOCH - 18 : training on 592830 raw words (371547 effective words) took 0.4s, 867815 effective words/s\n",
      "2020-06-13 01:05:09,812 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:09,814 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:09,816 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:09,817 : INFO : EPOCH - 19 : training on 592830 raw words (371431 effective words) took 0.5s, 774001 effective words/s\n",
      "2020-06-13 01:05:10,255 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-06-13 01:05:10,257 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-06-13 01:05:10,259 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-06-13 01:05:10,260 : INFO : EPOCH - 20 : training on 592830 raw words (371536 effective words) took 0.4s, 851877 effective words/s\n",
      "2020-06-13 01:05:10,260 : INFO : training on a 11856600 raw words (7429884 effective words) took 8.6s, 861264 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(tok_sents, size=200, window=10, min_count=15, sg=0, iter=20, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2725"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learned vocabulary\n",
    "words = list(w2v_model.wv.vocab)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-13 01:05:14,025 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hail', 0.8739173412322998),\n",
       " ('princess', 0.7359708547592163),\n",
       " ('queen', 0.7139188647270203),\n",
       " ('health', 0.6875106692314148),\n",
       " ('atta', 0.6720589995384216),\n",
       " ('gift', 0.6645910143852234),\n",
       " ('beloved', 0.6577285528182983),\n",
       " ('arendelle', 0.6089876294136047),\n",
       " ('yudhishtir', 0.5904470682144165),\n",
       " ('king', 0.583905041217804)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(['aurora'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aurora', 0.6322866678237915),\n",
       " ('hail', 0.629346489906311),\n",
       " ('queen', 0.6060227751731873),\n",
       " ('gods', 0.5343604683876038),\n",
       " ('england', 0.4813360869884491),\n",
       " ('gift', 0.47157424688339233),\n",
       " ('health', 0.45906397700309753),\n",
       " ('arendelle', 0.44755175709724426),\n",
       " ('duryodhan', 0.4470111131668091),\n",
       " ('arrived', 0.4429568648338318)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(['king', 'princess'], ['prince'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37860382"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('king', 'majesty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constructin matrix for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2725"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = set(w2v_model.wv.vocab)\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2591"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = voc.difference(en_stopwords)\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "indici = dict()\n",
    "for index, key in enumerate(voc):\n",
    "    if key not in indici:\n",
    "        indici[key] = index\n",
    "        \n",
    "reverse_indici = dict()\n",
    "for term, termID in indici.items():\n",
    "    reverse_indici[termID] = term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2591"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#costruzione matrice\n",
    "graph_matrix = np.zeros(shape=(len(voc), len(voc)), dtype=np.float32)\n",
    "\n",
    "for index1, key1 in enumerate(voc):\n",
    "    for index2, key2 in enumerate(voc):\n",
    "        graph_matrix[index1 ][index2] = w2v_model.wv.similarity(key1, key2)\n",
    "        \n",
    "len(graph_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes: 276\n",
      "links: 506\n"
     ]
    }
   ],
   "source": [
    "#constructing the list of tuples\n",
    "graph_list = []\n",
    "nodes = set() # <-- NUOVO\n",
    "for x in range(len(graph_matrix)):\n",
    "    for y in range(len(graph_matrix)):\n",
    "        if graph_matrix[x][y] >= 0.7 and x != y:\n",
    "            nodes |= set([reverse_indici[x], reverse_indici[y]])\n",
    "            graph_list.append((reverse_indici[x],reverse_indici[y],graph_matrix[x][y]))      \n",
    "print('nodes:', len(nodes))\n",
    "print('links:', len(graph_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUOVO\n",
    "graph = dict()\n",
    "graph[\"nodes\"] = [{\"id\": t} for t in nodes]\n",
    "graph[\"links\"] = [{\"source\":t[0], \"target\":t[1], \"value\":float(t[2])} for t in graph_list]\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(graph, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fast_model = FastText(tok_sents,size=100, window=10, min_count=5, sg=1, iter=20, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model.wv.most_similar(['apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model.wv.most_similar(['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model.wv.most_similar(['mirror'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_wAp_analogy = fast_model.wv.evaluate_word_analogies(test_file, dummy4unknown=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# google precomputed word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "google_w2v_file = 'GoogleNews-vectors-negative300.bin'\n",
    "google_model = gensim.models.KeyedVectors.load_word2vec_format(google_w2v_file,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model.most_similar(['king','female'],['male'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_large_analogy = google_model.wv.accuracy(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
